<!doctype html>
<html lang="en">
 <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Reinforcement Learning Assignment: About &amp; Credits</title>
  <meta name="description" content="Assignment comparing Passive ADP and Q-Learning on a 20x20 rover grid using REINFORCEjs. Credits to the library and RL sources.">
  <meta name="author" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- jquery and jqueryui -->
  <script src="https://code.jquery.com/jquery-2.1.3.min.js"></script>
  
  <!-- bootstrap -->
  <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
  <link href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css" rel="stylesheet">

  <!-- markdown -->
  <script type="text/javascript" src="external/marked.js"></script>
  <script type="text/javascript" src="external/highlight.pack.js"></script>
  <link rel="stylesheet" href="external/highlight_default.css">
  <script>hljs.initHighlightingOnLoad();</script>
  
  <!-- GA -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-3698471-24', 'auto');
  ga('send', 'pageview');
  </script>

  <style>
  #wrap {
    max-width: 800px;
    width: 100%;
    margin-left: auto;
    margin-right: auto;
    padding: 0 12px;
  }
  #exp pre {
    overflow-x: auto;
  }
  #exp img {
    max-width: 100%;
    height: auto;
  }
  @media (max-width: 768px) {
    #wrap { padding: 0 8px; }
    #mynav h1 { font-size: 24px !important; }
    #mynav img { width: 36px !important; height: 36px !important; }
    .nav.nav-pills { display: flex; flex-wrap: wrap; gap: 4px; }
    .nav.nav-pills li { margin: 2px 0; }
    .library-overview-img {
      float: none !important;
      max-width: 100% !important;
      margin: 12px 0 !important;
      text-align: center;
    }
    .library-overview-img img { max-width: 100% !important; }
  }
  </style>

  <script>
  function start() {
    $(".md").each(function(){
      $(this).html(marked($(this).html()));
    });
  }
  </script>
 </head>
 <body onload="start();">

   <div id="wrap">
   
   <div id="mynav" style="border-bottom:1px solid #999; padding-bottom: 10px; margin-bottom:50px;">
    <div>
      <img src="loop.svg" style="width:50px;height:50px;float:left;">
      <h1 style="font-size:50px;">REINFORCE<span style="color:#058;">js</span></h1>
    </div>
    <ul class="nav nav-pills">
      <li role="presentation" class="active"><a href="index.html">About</a></li>
      <li role="presentation"><a href="rover_assignment.html">Rover 20x20</a></li>
    </ul>
   </div>

   <div id="exp" class="md">

# About this assignment

This project implements a comparison of **Passive ADP** (Adaptive Dynamic Programming) and **Q-Learning** on a 20×20 rover grid. I built the assignment on top of an existing Reinforcement Learning library and extended it with a custom environment, trajectory generation, and comparison tools. Below I give credit to the sources I used and explain what I learned and why I chose this stack.

# Credits and sources

- **REINFORCEjs** — I used the [REINFORCEjs](https://github.com/karpathy/reinforcejs) library by [Andrej Karpathy](https://twitter.com/karpathy). It provides tabular TD learning (including Q-Learning via `RL.TDAgent`) and the building blocks for model-based methods. I extended the codebase with the rover grid environment, passive ADP agent (transition model from counts), and the assignment UI (trajectories, comparison charts, save/restore experience). The original library is MIT-licensed; full source is on [Github](https://github.com/karpathy/reinforcejs).

- **Sutton &amp; Barto** — I relied on *Reinforcement Learning: An Introduction* (Sutton and Barto) for the concepts: MDPs, value functions, Bellman equations, model-based vs model-free learning, and Q-Learning. This textbook was the main reference for understanding passive vs active learning and for implementing the assignment logic.

- **Library algorithms** — The assignment uses the library’s **tabular TD agent** (Q-Learning) and **DP-related ideas**: I implemented a passive ADP agent that estimates the transition model from (s, a, s′) counts and derives a policy from that model, contrasting it with the library’s active Q-Learning agent that learns directly from experience without a model.

# What I learned

- **Model-based vs model-free:** Passive ADP is model-based (learn transition model, then plan); Q-Learning is model-free (learn Q-values directly from experience). I learned how to compare them on the same grid and same evaluation pairs.

- **Passive vs active learning:** Passive ADP uses a fixed (e.g. random) policy and only learns the world model; Q-Learning actively improves the policy while exploring. I saw how this affects sample efficiency and success rate with limited trajectories.

- **Practical setup:** I learned how to define a grid MDP (states, actions, stochastic transitions), generate trajectories, compare policies via path length and success rate, and visualize transition probability matrices (TPM) and comparison charts.

# Why I chose REINFORCEjs

- **Browser-based and easy to run** — No install; open the HTML and run the assignment in any modern browser.

- **Right algorithms** — The library includes `RL.TDAgent` (Q-Learning) and the code structure made it straightforward to add a passive ADP agent and a custom grid environment (e.g. `RoverGrid20`).

- **Good fit for the problem** — Finite state/action grid MDPs are exactly what REINFORCEjs’s tabular agents are designed for. I could focus on the assignment design (trajectories, comparison, save/restore) instead of implementing Q-Learning from scratch.

- **Educational** — The library is well-known in RL education, so using it made it easy to attribute sources and to align the assignment with standard textbook material (Sutton &amp; Barto).

# Library overview (REINFORCEjs)

The following describes the **original REINFORCEjs library** (by Karpathy) that this assignment builds on. It includes Dynamic Programming, Tabular TD (SARSA/Q-Learning), Deep Q-Learning, and Policy Gradient methods. For this assignment I used mainly the tabular TD agent and the environment/agent APIs.

<div class="library-overview-img" style="text-align:justify; margin: 20px; float:right; max-width:300px;">
<img src="img/dpsolved.jpeg" style="max-width:300px;"><br>
Right: A simple Gridworld solved with Dynamic Programming (library demo).
</div>

- **Dynamic Programming** — For finite MDPs with known dynamics; tabular value iteration / policy iteration.
- **Tabular TD** — SARSA and Q-Learning; no environment model required; learns from experience.
- **Deep Q-Learning** — Q(s,a) approximated by a neural network (e.g. Mnih et al.); not used in this assignment.
- **Policy Gradients** — REINFORCE and deterministic policy gradients; library includes them but they are experimental.

# How the library is included

The assignment page loads the library as follows (from the original REINFORCEjs repo):

<pre><code class="html">&lt;script type="text/javascript" src="lib/rl.js"&gt;&lt;/script&gt;</code></pre>

The full source code of REINFORCEjs is on <a href="https://github.com/karpathy/reinforcejs">Github</a> under the MIT license. This assignment repository builds on that codebase and adds the rover 20×20 assignment content and demos.

<br><br><br><br><br>
   </div>

   </div>
 </body>
</html>